{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os.path\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm import tqdm\n",
    "from tqdm.dask import TqdmCallback\n",
    "\n",
    "from shared_code.utility.spark.set_environ import set_azure_env\n",
    "\n",
    "cb = TqdmCallback(desc=\"global\")\n",
    "cb.register()\n",
    "\n",
    "tqdm.pandas()\n",
    "tqdm.pandas(desc=\"global\")\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "set_azure_env()\n",
    "\n",
    "from shared_code.utility.storage.azure_file_storage import AzureFileStorageAdapter\n",
    "\n",
    "fs_adapter = AzureFileStorageAdapter('data')\n",
    "file_system = fs_adapter.get_file_storage()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSources:\n",
    "\tname: str\n",
    "\tdata: List[str]\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_dict(obj: dict) -> 'DataSources':\n",
    "\t\t_name = obj.get(\"name\")\n",
    "\t\t_data = [x for x in obj.get(\"data\")]\n",
    "\t\treturn DataSources(_name, _data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_source(x: object, source_list) -> str:\n",
    "\tfor source in source_list:\n",
    "\t\tif x['subreddit'] in source['data']:\n",
    "\t\t\treturn source['name']\n",
    "\treturn \"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_compressed_data(x: object) -> str:\n",
    "\ttry:\n",
    "\t\tdata = open(x['path'], 'rb').read()\n",
    "\t\treturn data\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn \"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "all_data = pd.read_parquet(\"data/processed_raw_data.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "display(all_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "filtered_on_exist = all_data.where(lambda x: x['exists']).dropna(how='all').reset_index(drop=True)\n",
    "filtered_on_exist = filtered_on_exist.where(lambda x: x['caption'] != \"\").dropna(how='all').reset_index(drop=True)\n",
    "display(filtered_on_exist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sources = [\n",
    "\t{\"name\": \"CityDiffusion\", \"data\": [\"CityPorn\"]},\n",
    "\t{\"name\": \"NatureDiffusion\", \"data\": [\"EarthPorn\"]},\n",
    "\t{\"name\": \"CosmicDiffusion\", \"data\": [\"spaceporn\"]},\n",
    "\t{\"name\": \"ITAPDiffusion\", \"data\": [\"itookapicture\"]},\n",
    "\t{\"name\": \"MemeDiffusion\", \"data\": [\"memes\",\"trippinthroughtime\"]},\n",
    "\t{\"name\": \"SexyDiffusion\", \"data\": [\"sfwpetite\",\"selfies\",\"Amicute\",\"amihot\",\"AmIhotAF\",\"HotGirlNextDoor\"]},\n",
    "\t{\"name\": \"FatSquirrelDiffusion\", \"data\": [\"fatsquirrelhate\"]},\n",
    "\t{\"name\": \"RedHeadDiffusion\", \"data\": [\"SFWRedheads\"]},\n",
    "\t{\"name\": \"NextDoorGirlsDiffusion\", \"data\": [\"SFWNextDoorGirls\"]}\n",
    "]\n",
    "sources_df = pd.DataFrame.from_records(sources)\n",
    "display(sources_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "smaller_exportable_df = pd.DataFrame(\n",
    "\tdata=[filtered_on_exist['path'], filtered_on_exist['image_name'], filtered_on_exist['caption'], filtered_on_exist['title'], filtered_on_exist['subreddit']],\n",
    "\tindex=['path', 'image_name', 'caption', 'title', 'subreddit']).T\n",
    "\n",
    "with ProgressBar():\n",
    "\tsmaller_exportable_df['name'] = smaller_exportable_df.progress_apply(lambda x: add_source(x, sources), axis=1)\n",
    "\n",
    "display(smaller_exportable_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with cb:\n",
    "\tdask_frame = dd.from_pandas(smaller_exportable_df, npartitions=10)\n",
    "\tsmaller_exportable_df['image_data'] = dask_frame.apply(lambda x: add_compressed_data(x), meta=('str', object), axis=1).compute()\n",
    "\n",
    "display(smaller_exportable_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "filtered_again = smaller_exportable_df.where(lambda x: x['image_data'] != \"\").dropna(how='all').reset_index(drop=True)\n",
    "filtered_again = filtered_again.where(lambda x: x['name'] != \"\").dropna(how='all').reset_index(drop=True)\n",
    "display(filtered_again)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final = filtered_again\n",
    "display(final)\n",
    "\n",
    "grouped = final.groupby('name')\n",
    "groupings = [grouped.get_group(x) for x in grouped.groups]\n",
    "\n",
    "for group in groupings:\n",
    "\tgroup.to_parquet(f\"data/curated/{group['name'].iloc[0]}.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lines = []\n",
    "training_lines = []\n",
    "for record in final.to_dict(orient='records'):\n",
    "\tsubreddit = record['subreddit']\n",
    "\tname = record['name']\n",
    "\tprompt = record['title']\n",
    "\tcaption = record['caption']\n",
    "\tline = f\"<|startoftext|><|model|>{name}<|prompt|>{prompt}<|text|>{caption}<|endoftext|>\" + \"\\n\"\n",
    "\tlines.append(line)\n",
    "with open(\"training.txt\", \"wb\") as f:\n",
    "\tfor line in lines:\n",
    "\t\tf.write(line.encode(\"utf-8\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_paths = os.listdir(\"data/curated\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for path in data_paths:\n",
    "\tfoo = os.path.join(\"data\", \"curated\", path)\n",
    "\tdf = pd.read_parquet(foo)\n",
    "\tdfs.append(df)\n",
    "\n",
    "final = pd.concat(dfs)\n",
    "display(final)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
