{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers@main > /dev/null\n",
    "!pip install accelerate  > /dev/null\n",
    "!pip install  simpletransformers==0.63.3  > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "import torch\n",
    "import gc\n",
    "import pandas\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import logging\n",
    "import pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"FunnyGuy\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parent_directory = \"/content/drive/MyDrive/RawData\"\n",
    "\n",
    "data_dir = f\"{parent_directory}/data/\"\n",
    "\n",
    "model_output_dir = f\"{parent_directory}/{model_name}\"\n",
    "\n",
    "tokenizer_path = f\"{model_output_dir}\"\n",
    "\n",
    "training_data_path = f\"{data_dir}/{model_name}-training.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def has_valid_line(input_line: str) -> bool:\n",
    "    bad_words_found = []\n",
    "    black_list = [\"[removed]\", \"[deleted]\"]\n",
    "    for word in black_list:\n",
    "        if input_line.lower().__contains__(word.lower()):\n",
    "            print(f\":: Line contains word {word}... Skipping\")\n",
    "            bad_words_found.append(input_line)\n",
    "\n",
    "    return len(bad_words_found) == 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "def token_length_appropriate(prompt) -> bool:\n",
    "    \"\"\"\n",
    "    Ensures that the total number of encoded tokens is within acceptable limits.\n",
    "    :param tokenizer: An instance of the tokenizer being used.\n",
    "    :param prompt: UTF-8 Text that is assumed to have been processed.\n",
    "    :return: True if acceptable.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    if len(tokens) > 1024:\n",
    "        print(f\":: Tokens for model input is > {1024}. Skipping input\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pandas.read_csv(training_data_path)\n",
    "\n",
    "conversations = list(df['TrainingString'])\n",
    "\n",
    "valid_lines = []\n",
    "\n",
    "for conversation in conversations:\n",
    "    if token_length_appropriate(conversation) and has_valid_line(conversation):\n",
    "        valid_lines.append(conversation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "\n",
    "generator.manual_seed(0)\n",
    "\n",
    "print(f\":: Total Number Of Samples {len(valid_lines)}\")\n",
    "\n",
    "train_size = int(0.9 * len(valid_lines))\n",
    "\n",
    "train_dataset_file, eval_dataset_file = random_split(list(valid_lines), [train_size, len(valid_lines) - train_size], generator=generator)\n",
    "\n",
    "with open(\"train.txt\", 'w', encoding=\"utf-8\") as train_out, open(\"eval.txt\", \"w\", encoding=\"utf-8\") as eval_out:\n",
    "        train_out.writelines([repr(line)[1:-1] + \"<|endoftext|>\" + \"\\n\" for line in train_dataset_file])\n",
    "        eval_out.writelines([repr(line)[1:-1] + \"<|endoftext|>\" + \"\\n\" for line in eval_dataset_file])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "args = {\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gradient_accumulation_steps\": 100,\n",
    "    \"dataset_type\": \"simple\",\n",
    "    \"sliding_window\": True,\n",
    "    \"max_seq_length\": 1024,\n",
    "\t\"mlm\": False, # has to be false for gpt-2\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"use_cached_eval_features\": True,\n",
    "    \"evaluate_during_training_verbose\": True,\n",
    "    \"save_optimizer_and_scheduler\": False,\n",
    "    \"save_eval_checkpoints\": False,\n",
    "    \"save_model_every_epoch\": True,\n",
    "    \"save_steps\": -1,\n",
    "    \"train_batch_size\":3,\n",
    "    \"num_train_epochs\":12,\n",
    "    \"output_dir\": f\"{model_output_dir}/\",\n",
    "\t\"best_model_dir\": f\"{model_output_dir}/best_model\"\n",
    "}\n",
    "model = LanguageModelingModel(\"gpt2\", \"gpt2-medium\", args=args)\n",
    "model.train_model(train_file=\"train.txt\", eval_file=\"eval.txt\", args=args, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def capture_tag(test_string: str, expected_tag: str):\n",
    "    regex = r\"\\<\\|(.*)\\|\\>\"\n",
    "\n",
    "    matches = re.finditer(regex, test_string, re.MULTILINE)\n",
    "\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "\n",
    "        print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n",
    "        if match.group() == expected_tag:\n",
    "            print(f\"{match.group()} {expected_tag}\")\n",
    "            return_string = test_string.replace(match.group(), \"\")\n",
    "            return return_string\n",
    "\n",
    "        for groupNum in range(0, len(match.groups())):\n",
    "            groupNum = groupNum + 1\n",
    "\n",
    "            print (\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from simpletransformers.language_generation import LanguageGenerationModel, LanguageGenerationArgs\n",
    "\n",
    "text_model_generator = LanguageGenerationModel(\"gpt2\",  f\"{model_output_dir}/best_model\", args = {\n",
    "\t\t'max_length': 1000,\n",
    "\t\t'num_return_sequences': 1,\n",
    "\t\t'repetition_penalty': 1.01,\n",
    "\t\t'stop_token': '<|endoftext|>',\n",
    "\t\t'temperature': 0.8,\n",
    "\t\t'top_k': 40,\n",
    "\t})\n",
    "\n",
    "\n",
    "prompt = \"<|soss r/dalle2|><|sot|>Detailed scientific diagram depicting the anatomy of a tomato, full colour, realistic<|sost|>https://i.imgur.com/7adBOXn.jpg<|sor u/AsterJ|>It's going to be sad day when it learns to properly spell.  I feel like this era is a fleeting moment in AI history.  We must cherish it.<|eor|><|sor|>\"\n",
    "\n",
    "body = \"The religion of Barryism always enjoyed decent popularity on this sub, but after the destruction of u/Creator_GUTENMAN and fall of GUTENMANism it began to gow astoundingly fast. What is your opinion on the religion of Barryism? Will it become the dominant religion of r/subsimgpt2interactive?\"\n",
    "prompt = f\"<|soss r/subsimgpt2interactive|><|sot|>{body}<|eot|><|sost|><|eost|><|sor|>\"\n",
    "\n",
    "import re\n",
    "regex = r\"\\<\\|(.*)\\|\\>\"\n",
    "\n",
    "\n",
    "reply = None\n",
    "refresh_args = {\n",
    "\t\t'max_length': 1000,\n",
    "\t\t'num_return_sequences': 1,\n",
    "\t\t'repetition_penalty': 1.01,\n",
    "\t\t'stop_token': '<|endoftext|>',\n",
    "\t\t'temperature': 0.8,\n",
    "\t\t'top_k': 40,\n",
    "\t}\n",
    "while reply is None:\n",
    "\tfor text in text_model_generator.generate(prompt=prompt, args=refresh_args, verbose=True):\n",
    "\t\tfoo = text.replace(prompt, \"\\n\")\n",
    "\t\tresult = capture_tag(foo, \"<|eor|>\")\n",
    "\t\tif result != None:\n",
    "\t\t\treply = result\n",
    "\t\t\tbreak\n",
    "print(reply)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
