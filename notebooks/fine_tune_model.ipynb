{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bot_label = 'experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_file = \"filtered_training.txt\"\n",
    "eval_file = \"filtered_eval.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = LanguageModelingArgs()\n",
    "model_args.model_type = \"gpt2\"\n",
    "model_args.model_name = \"gpt2-medium\"\n",
    "model_args.optimizer = \"AdamW\"\n",
    "model_args.overwrite_output_dir=True\n",
    "\n",
    "model_args.learning_rate=1e-4\n",
    "model_args.train_batch_size=1\n",
    "model_args.num_train_epochs=5\n",
    "model_args.gradient_accumulation_steps=3\n",
    "model_args.max_steps=-1\n",
    "model_args.evaluate_during_training_steps=1000\n",
    "\n",
    "model_args.dataset_type = \"simple\"\n",
    "model_args.sliding_window=True\n",
    "model_args.max_seq_length=512\n",
    "model_args.mlm=False\n",
    "model_args.evaluate_during_training=True\n",
    "model_args.use_cached_eval_features=True\n",
    "model_args.valuate_during_training_verbose=True\n",
    "model_args.save_optimizer_and_scheduler=False\n",
    "model_args.save_eval_checkpoints=True\n",
    "model_args.save_model_every_epoch=False\n",
    "model_args.save_steps=-1\n",
    "model_args.n_gpus= 2\n",
    "model_args.output_dir=f\"{bot_label}/\"\n",
    "model_args.best_model_dir=f\"{bot_label}/best_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{bot_label}-training_parameters.txt\", 'w') as f:\n",
    "    f.write(model_args.__repr__())\n",
    "\n",
    "print(model_args.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LanguageModelingModel(model_type=model_args.model_type, model_name=model_args.model_name, args=model_args, use_cuda=True, cuda_device=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/233091 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49adaad3d68d4d43938b755b7471de4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspaces\\envs\\research\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8df56d74e9de4409bcf85f74c366b2d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running Epoch 0 of 5:   0%|          | 0/271732 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb993d0151904340bb7e443c0ace7b9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspaces\\envs\\research\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/23310 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f67c6526218b4683ac1b7fdbba5d53c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train_model(train_file=training_file, eval_file=eval_file, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}