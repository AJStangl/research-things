{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from shared_code.utility.spark.set_environ import set_azure_env\n",
    "\n",
    "set_azure_env()\n",
    "\n",
    "from shared_code.utility.storage.table import TableAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.dask import TqdmCallback\n",
    "\n",
    "cb = TqdmCallback(desc=\"global\")\n",
    "cb.register()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"global\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class InnerProgressBar(tqdm):\n",
    "\tdef __init__(self, total, desc):\n",
    "\t\tsuper().__init__(desc=desc)\n",
    "\t\tself.total = total\n",
    "\t\tself.current = 0\n",
    "\n",
    "\tdef update_to(self):\n",
    "\t\tself.update(self.current)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"/data/parquet/\"\n",
    "parquet_process_data_path = data_path + \"processed_data.parquet\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating SD Models\n",
    "## SexyDiffusion\n",
    "- HotGirlNextDoor\n",
    "- sfwpetite\n",
    "- AmIhotAF\n",
    "- selfies\n",
    "- amihot\n",
    "- SFWNextDoorGirls\n",
    "- SFWRedheads\n",
    "- SFWPetite\n",
    "- Amicute\n",
    "\n",
    "## CityScapes\n",
    "- CityPorn\n",
    "\n",
    "## NatureScapes\n",
    "- EarthPorn\n",
    "\n",
    "## Memes\n",
    "- greentext\n",
    "\n",
    "The basic training line for a model is:\n",
    "```json lines\n",
    "{\"file_name\": \"0001.png\", \"text\": \"A cute cat.\"}\n",
    "{\"file_name\": \"0002.png\", \"text\": \"A cute dog.\"}\n",
    "{\"file_name\": \"0003.png\", \"text\": \"A cute bird.\"}\n",
    "```\n",
    "\n",
    "For each image we will do the following:\n",
    "- Caption the image with the caption (the actual caption from the other AI)\n",
    "- Use the thumbnail version of the image is to be used\n",
    "- Move all the images to a single folder along with the metadata.jsonl file\n",
    "\n",
    "A training line will look like:\n",
    "```json lines\n",
    "{\"file_name\": \"0001.png\", \"text\": \"A cute cat.\"}\n",
    "```\n",
    "\n",
    "Create small GPT Model for each SD model that will be used to generate the captions for the images based on what a user would say with the following translation:\n",
    "\n",
    "`<|startoftext|><|model|>SexyDiffusion<|model|><|prompt|>A cute cat<|prompt|><|text|>Foo<|text|><endoftext|>`\n",
    "\n",
    "This file will be named and stored in the following format:\n",
    "`training.txt`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Reading from parquet {parquet_process_data_path} with Updated Thumbnail Captions\")\n",
    "processed_with_captions_more = pandas.read_parquet(parquet_process_data_path)\n",
    "display(processed_with_captions_more)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Filtering Subreddits with Images By original_caption\")\n",
    "filtered_captions = processed_with_captions_more[\n",
    "\t(processed_with_captions_more[\"original_caption\"] != \"bruh\") &\n",
    "\t(~processed_with_captions_more[\"original_caption\"].isna() | ~ processed_with_captions_more[\n",
    "\t\t\"original_caption\"].isnull())\n",
    "\t]\n",
    "\n",
    "filtered_captions_display = filtered_captions.groupby(\"subreddit\").size().reset_index(name=\"count\")\n",
    "\n",
    "display(filtered_captions_display.sort_values(\"count\", ascending=False))\n",
    "print(f\"Total Records {filtered_captions_display['count'].sum()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Filtering Subreddits with Images By thumbnail_caption\")\n",
    "filtered_captions_by_thumbnail = filtered_captions[\n",
    "\t(processed_with_captions_more[\"thumbnail_caption\"] != \"bruh\") &\n",
    "\t(~processed_with_captions_more[\"thumbnail_caption\"].isna() | ~ processed_with_captions_more[\n",
    "\t\t\"thumbnail_caption\"].isnull())\n",
    "\t]\n",
    "\n",
    "filtered_captions_by_thumbnail_display = filtered_captions_by_thumbnail.groupby(\"subreddit\").size().reset_index(\n",
    "\tname=\"count\")\n",
    "display(filtered_captions_by_thumbnail_display.sort_values(\"count\", ascending=False))\n",
    "print(f\"Total Records {filtered_captions_by_thumbnail_display['count'].sum()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sources = [\n",
    "\t{\"name\": \"CityScapes\", \"data\": [\"CityPorn\"]},\n",
    "\t{\"name\": \"NatureScapes\", \"data\": [\"EarthPorn\"]},\n",
    "\t{\"name\": \"memes\", \"data\": [\"greentext\"]},\n",
    "\t{\"name\": \"SexyDiffusion\",\n",
    "\t \"data\": [\"HotGirlNextDoor\", \"sfwpetite\", \"AmIhotAF\", \"selfies\", \"amihot\", \"SFWNextDoorGirls\", \"SFWRedheads\",\n",
    "\t\t\t  \"SFWPetite\", \"Amicute\"]}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "for item in sources:\n",
    "\tnew_records = []\n",
    "\tout_dir = os.path.join(\"out\", item['name'])\n",
    "\tos.makedirs(out_dir, exist_ok=True)\n",
    "\tfor record in filtered_captions_by_thumbnail.to_dict(orient='records'):\n",
    "\t\tsubreddit = record['subreddit']\n",
    "\t\tif subreddit in item['data']:\n",
    "\t\t\tvalid_image = record.get(\"thumbnail_path\")\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfoo:Image = Image.open(valid_image)\n",
    "\t\t\t\tb = foo.size\n",
    "\t\t\t\tfoo.close()\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(f\"Invalid Image {valid_image}\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tshutil.copy(valid_image, out_dir)\n",
    "\t\t\tout_record = {\"file_name\": record.get(\"file_name\"), \"text\": record.get(\"original_caption\")}\n",
    "\t\t\tnew_records.append(out_record)\n",
    "\n",
    "\tout_records = pandas.DataFrame(new_records)\n",
    "\tout_records.to_json(\"metadata.jsonl\", orient=\"records\", lines=True)\n",
    "\tshutil.move(\"metadata.jsonl\", out_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(\"out.zip\"):\n",
    "    print(\"Removing Old File\")\n",
    "    !rm out.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!tar -a -c -f out.zip out"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
