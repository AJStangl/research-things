{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm.dask import TqdmCallback\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from shared_code.utility.spark.set_environ import set_azure_env\n",
    "\n",
    "set_azure_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "cb = TqdmCallback(desc=\"global-dd\")\n",
    "cb.register()\n",
    "tqdm.pandas(desc=\"global-pd\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class InnerProgressBar(tqdm):\n",
    "\tdef __init__(self, total, desc):\n",
    "\t\tsuper().__init__(desc=desc)\n",
    "\t\tself.total = total\n",
    "\t\tself.current = 0\n",
    "\n",
    "\tdef update_to(self):\n",
    "\t\tself.update(self.current)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_path = \"/data/parquet/\"\n",
    "parquet_process_data_path = data_path + \"processed_data.parquet\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating SD Models\n",
    "## SexyDiffusion\n",
    "- HotGirlNextDoor\n",
    "- sfwpetite\n",
    "- AmIhotAF\n",
    "- selfies\n",
    "- amihot\n",
    "- SFWNextDoorGirls\n",
    "- SFWRedheads\n",
    "- SFWPetite\n",
    "- Amicute\n",
    "\n",
    "## CityScapes\n",
    "- CityPorn\n",
    "\n",
    "## NatureScapes\n",
    "- EarthPorn\n",
    "\n",
    "## Memes\n",
    "- greentext\n",
    "\n",
    "The basic training line for a model is:\n",
    "```json lines\n",
    "{\"file_name\": \"0001.png\", \"text\": \"A cute cat.\"}\n",
    "{\"file_name\": \"0002.png\", \"text\": \"A cute dog.\"}\n",
    "{\"file_name\": \"0003.png\", \"text\": \"A cute bird.\"}\n",
    "```\n",
    "\n",
    "For each image we will do the following:\n",
    "- Caption the image with the caption (the actual caption from the other AI)\n",
    "- Use the thumbnail version of the image is to be used\n",
    "- Move all the images to a single folder along with the metadata.jsonl file\n",
    "\n",
    "A training line will look like:\n",
    "```json lines\n",
    "{\"file_name\": \"0001.png\", \"text\": \"A cute cat.\"}\n",
    "```\n",
    "\n",
    "Create small GPT Model for each SD model that will be used to generate the captions for the images based on what a user would say with the following translation:\n",
    "\n",
    "`<|startoftext|><|model|>SexyDiffusion<|model|><|prompt|>A cute cat<|prompt|><|text|>Foo<|text|><endoftext|>`\n",
    "\n",
    "This file will be named and stored in the following format:\n",
    "`training.txt`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from parquet /data/parquet/processed_data.parquet with Updated Thumbnail Captions\n"
     ]
    },
    {
     "data": {
      "text/plain": "                subreddit          file_name  \\\n0                CityPorn  4emw5uldib9a1.jpg   \n3                AmIhotAF  4xyb1vgbjb9a1.jpg   \n4               greentext  3mewbe0wjb9a1.jpg   \n5               spaceporn  7s5aafaqkb9a1.jpg   \n7               spaceporn  abojw7lqlb9a1.jpg   \n...                   ...                ...   \n11724           spaceporn  abwhhq0w8b9a1.jpg   \n11725           spaceporn  7hzipg1bab9a1.jpg   \n11726           greentext        bgho6WK.jpg   \n11728  trippinthroughtime        arCpzQ0.jpg   \n11730     HotGirlNextDoor  p6yewrl7eb9a1.jpg   \n\n                                                    text  \\\n0                                    New York in the fog   \n3                         Just looking for entertainment   \n4                                    Anon wants Elon cut   \n5                          Northern Lights above Lofoten   \n7                                          Viking Lights   \n...                                                  ...   \n11724           Polaris to Cassiopeia on a cloudy night.   \n11725  The hunt for habitable ocean worlds beyond our...   \n11726                        Anon does a little trolling   \n11728         He didn't shed light on the topic I guess.   \n11730                                             (IKTR)   \n\n                                          thumbnail_path  thumbnail_exists  \\\n0      D:\\data\\images\\CityPorn\\thumbnail\\4emw5uldib9a...              True   \n3      D:\\data\\images\\AmIhotAF\\thumbnail\\4xyb1vgbjb9a...              True   \n4      D:\\data\\images\\greentext\\thumbnail\\3mewbe0wjb9...              True   \n5      D:\\data\\images\\spaceporn\\thumbnail\\7s5aafaqkb9...              True   \n7      D:\\data\\images\\spaceporn\\thumbnail\\abojw7lqlb9...              True   \n...                                                  ...               ...   \n11724  D:\\data\\images\\spaceporn\\thumbnail\\abwhhq0w8b9...              True   \n11725  D:\\data\\images\\spaceporn\\thumbnail\\7hzipg1bab9...              True   \n11726     D:\\data\\images\\greentext\\thumbnail\\bgho6WK.jpg              True   \n11728  D:\\data\\images\\trippinthroughtime\\thumbnail\\ar...              True   \n11730  D:\\data\\images\\HotGirlNextDoor\\thumbnail\\p6yew...              True   \n\n                                         original_image  \\\n0             D:\\data\\images\\CityPorn\\4emw5uldib9a1.jpg   \n3             D:\\data\\images\\AmIhotAF\\4xyb1vgbjb9a1.jpg   \n4            D:\\data\\images\\greentext\\3mewbe0wjb9a1.jpg   \n5            D:\\data\\images\\spaceporn\\7s5aafaqkb9a1.jpg   \n7            D:\\data\\images\\spaceporn\\abojw7lqlb9a1.jpg   \n...                                                 ...   \n11724        D:\\data\\images\\spaceporn\\abwhhq0w8b9a1.jpg   \n11725        D:\\data\\images\\spaceporn\\7hzipg1bab9a1.jpg   \n11726              D:\\data\\images\\greentext\\bgho6WK.jpg   \n11728     D:\\data\\images\\trippinthroughtime\\arCpzQ0.jpg   \n11730  D:\\data\\images\\HotGirlNextDoor\\p6yewrl7eb9a1.jpg   \n\n       original_image_exists                              hash       id  \\\n0                       True  7a8d96e378c15c8ab8440ac311f12c11  1000cej   \n3                       True  e554c1ed7ffa2740436ac082068b2824  1000glf   \n4                       True  1dec3dabb5e46cde01855d06089c287a  1000j1n   \n5                       True  2c39ce1290fba541abd0b004b09da6b2  1000mjs   \n7                       True  0f72de47c69ff50eca5fa3990215f4ac  1000qpd   \n...                      ...                               ...      ...   \n11724                   True  f5973637fc56360c15818ba0ca1f7ffa   zzz6dp   \n11725                   True  5b22bea7582229c1f9b992176a2ca2c6   zzzcn5   \n11726                   True  df666b8b2ad543c77b3fdba89becda1a   zzzeoi   \n11728                   True  5007b937974ae333022c0c91b795ca09   zzzlbf   \n11730                   True  94dea288ddffb51eb1a786d469b59374   zzzu28   \n\n                                        original_caption  \\\n0       a city street filled with lots of tall buildings   \n3      a beautiful young woman in a black dress posin...   \n4      a collage of photos showing a man in a suit an...   \n5            a nighttime view of a large rock formation    \n7           a boat sitting on top of a lush green field    \n...                                                  ...   \n11724   a large amount of light is reflected in the sky    \n11725  a blue and white photo of a blue and white obj...   \n11726  A man wearing a black shirt and black pants is...   \n11728                a man in a red shirt and a red hat    \n11730  A woman in a pink bathing suit is sitting on a...   \n\n                                       thumbnail_caption  \n0       a city street filled with lots of tall buildings  \n3      a beautiful young woman in a black dress posin...  \n4      a collage of photos showing a man in a suit an...  \n5            a nighttime view of a large rock formation   \n7           a boat sitting on top of a lush green field   \n...                                                  ...  \n11724   a large amount of light is reflected in the sky   \n11725  a blue and white photo of a blue and white obj...  \n11726  A man wearing a black shirt and black pants is...  \n11728                a man in a red shirt and a red hat   \n11730  A woman in a pink bathing suit is sitting on a...  \n\n[7901 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>file_name</th>\n      <th>text</th>\n      <th>thumbnail_path</th>\n      <th>thumbnail_exists</th>\n      <th>original_image</th>\n      <th>original_image_exists</th>\n      <th>hash</th>\n      <th>id</th>\n      <th>original_caption</th>\n      <th>thumbnail_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CityPorn</td>\n      <td>4emw5uldib9a1.jpg</td>\n      <td>New York in the fog</td>\n      <td>D:\\data\\images\\CityPorn\\thumbnail\\4emw5uldib9a...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\CityPorn\\4emw5uldib9a1.jpg</td>\n      <td>True</td>\n      <td>7a8d96e378c15c8ab8440ac311f12c11</td>\n      <td>1000cej</td>\n      <td>a city street filled with lots of tall buildings</td>\n      <td>a city street filled with lots of tall buildings</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AmIhotAF</td>\n      <td>4xyb1vgbjb9a1.jpg</td>\n      <td>Just looking for entertainment</td>\n      <td>D:\\data\\images\\AmIhotAF\\thumbnail\\4xyb1vgbjb9a...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\AmIhotAF\\4xyb1vgbjb9a1.jpg</td>\n      <td>True</td>\n      <td>e554c1ed7ffa2740436ac082068b2824</td>\n      <td>1000glf</td>\n      <td>a beautiful young woman in a black dress posin...</td>\n      <td>a beautiful young woman in a black dress posin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>greentext</td>\n      <td>3mewbe0wjb9a1.jpg</td>\n      <td>Anon wants Elon cut</td>\n      <td>D:\\data\\images\\greentext\\thumbnail\\3mewbe0wjb9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\greentext\\3mewbe0wjb9a1.jpg</td>\n      <td>True</td>\n      <td>1dec3dabb5e46cde01855d06089c287a</td>\n      <td>1000j1n</td>\n      <td>a collage of photos showing a man in a suit an...</td>\n      <td>a collage of photos showing a man in a suit an...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spaceporn</td>\n      <td>7s5aafaqkb9a1.jpg</td>\n      <td>Northern Lights above Lofoten</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\7s5aafaqkb9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\7s5aafaqkb9a1.jpg</td>\n      <td>True</td>\n      <td>2c39ce1290fba541abd0b004b09da6b2</td>\n      <td>1000mjs</td>\n      <td>a nighttime view of a large rock formation</td>\n      <td>a nighttime view of a large rock formation</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>spaceporn</td>\n      <td>abojw7lqlb9a1.jpg</td>\n      <td>Viking Lights</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\abojw7lqlb9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\abojw7lqlb9a1.jpg</td>\n      <td>True</td>\n      <td>0f72de47c69ff50eca5fa3990215f4ac</td>\n      <td>1000qpd</td>\n      <td>a boat sitting on top of a lush green field</td>\n      <td>a boat sitting on top of a lush green field</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11724</th>\n      <td>spaceporn</td>\n      <td>abwhhq0w8b9a1.jpg</td>\n      <td>Polaris to Cassiopeia on a cloudy night.</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\abwhhq0w8b9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\abwhhq0w8b9a1.jpg</td>\n      <td>True</td>\n      <td>f5973637fc56360c15818ba0ca1f7ffa</td>\n      <td>zzz6dp</td>\n      <td>a large amount of light is reflected in the sky</td>\n      <td>a large amount of light is reflected in the sky</td>\n    </tr>\n    <tr>\n      <th>11725</th>\n      <td>spaceporn</td>\n      <td>7hzipg1bab9a1.jpg</td>\n      <td>The hunt for habitable ocean worlds beyond our...</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\7hzipg1bab9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\7hzipg1bab9a1.jpg</td>\n      <td>True</td>\n      <td>5b22bea7582229c1f9b992176a2ca2c6</td>\n      <td>zzzcn5</td>\n      <td>a blue and white photo of a blue and white obj...</td>\n      <td>a blue and white photo of a blue and white obj...</td>\n    </tr>\n    <tr>\n      <th>11726</th>\n      <td>greentext</td>\n      <td>bgho6WK.jpg</td>\n      <td>Anon does a little trolling</td>\n      <td>D:\\data\\images\\greentext\\thumbnail\\bgho6WK.jpg</td>\n      <td>True</td>\n      <td>D:\\data\\images\\greentext\\bgho6WK.jpg</td>\n      <td>True</td>\n      <td>df666b8b2ad543c77b3fdba89becda1a</td>\n      <td>zzzeoi</td>\n      <td>A man wearing a black shirt and black pants is...</td>\n      <td>A man wearing a black shirt and black pants is...</td>\n    </tr>\n    <tr>\n      <th>11728</th>\n      <td>trippinthroughtime</td>\n      <td>arCpzQ0.jpg</td>\n      <td>He didn't shed light on the topic I guess.</td>\n      <td>D:\\data\\images\\trippinthroughtime\\thumbnail\\ar...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\trippinthroughtime\\arCpzQ0.jpg</td>\n      <td>True</td>\n      <td>5007b937974ae333022c0c91b795ca09</td>\n      <td>zzzlbf</td>\n      <td>a man in a red shirt and a red hat</td>\n      <td>a man in a red shirt and a red hat</td>\n    </tr>\n    <tr>\n      <th>11730</th>\n      <td>HotGirlNextDoor</td>\n      <td>p6yewrl7eb9a1.jpg</td>\n      <td>(IKTR)</td>\n      <td>D:\\data\\images\\HotGirlNextDoor\\thumbnail\\p6yew...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\HotGirlNextDoor\\p6yewrl7eb9a1.jpg</td>\n      <td>True</td>\n      <td>94dea288ddffb51eb1a786d469b59374</td>\n      <td>zzzu28</td>\n      <td>A woman in a pink bathing suit is sitting on a...</td>\n      <td>A woman in a pink bathing suit is sitting on a...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7901 rows × 11 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Reading from parquet {parquet_process_data_path} with Updated Thumbnail Captions\")\n",
    "processed_with_captions_more = pandas.read_parquet(parquet_process_data_path)\n",
    "display(processed_with_captions_more)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Subreddits with Images By original_caption\n"
     ]
    },
    {
     "data": {
      "text/plain": "             subreddit  count\n3            EarthPorn   1223\n2             CityPorn   1064\n4                Faces   1028\n7          SFWRedheads    839\n9            greentext    713\n6     SFWNextDoorGirls    665\n10               memes    647\n13           spaceporn    572\n14  trippinthroughtime    284\n12           sfwpetite    274\n5      HotGirlNextDoor    262\n0             AmIhotAF    183\n11             selfies     61\n8               amihot     53\n1              Amicute     33",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>EarthPorn</td>\n      <td>1223</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CityPorn</td>\n      <td>1064</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Faces</td>\n      <td>1028</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SFWRedheads</td>\n      <td>839</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>greentext</td>\n      <td>713</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SFWNextDoorGirls</td>\n      <td>665</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>memes</td>\n      <td>647</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>spaceporn</td>\n      <td>572</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>trippinthroughtime</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sfwpetite</td>\n      <td>274</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HotGirlNextDoor</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>AmIhotAF</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>selfies</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>amihot</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amicute</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records 7901\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering Subreddits with Images By original_caption\")\n",
    "filtered_captions = processed_with_captions_more[\n",
    "\t(processed_with_captions_more[\"original_caption\"] != \"bruh\") &\n",
    "\t(~processed_with_captions_more[\"original_caption\"].isna() | ~ processed_with_captions_more[\n",
    "\t\t\"original_caption\"].isnull())\n",
    "\t]\n",
    "\n",
    "filtered_captions_display = filtered_captions.groupby(\"subreddit\").size().reset_index(name=\"count\")\n",
    "\n",
    "display(filtered_captions_display.sort_values(\"count\", ascending=False))\n",
    "print(f\"Total Records {filtered_captions_display['count'].sum()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Subreddits with Images By thumbnail_caption\n"
     ]
    },
    {
     "data": {
      "text/plain": "             subreddit  count\n3            EarthPorn   1223\n2             CityPorn   1064\n4                Faces   1028\n7          SFWRedheads    839\n9            greentext    713\n6     SFWNextDoorGirls    665\n10               memes    647\n13           spaceporn    572\n14  trippinthroughtime    284\n12           sfwpetite    274\n5      HotGirlNextDoor    262\n0             AmIhotAF    183\n11             selfies     61\n8               amihot     53\n1              Amicute     33",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>EarthPorn</td>\n      <td>1223</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CityPorn</td>\n      <td>1064</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Faces</td>\n      <td>1028</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SFWRedheads</td>\n      <td>839</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>greentext</td>\n      <td>713</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SFWNextDoorGirls</td>\n      <td>665</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>memes</td>\n      <td>647</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>spaceporn</td>\n      <td>572</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>trippinthroughtime</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sfwpetite</td>\n      <td>274</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HotGirlNextDoor</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>AmIhotAF</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>selfies</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>amihot</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amicute</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records 7901\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering Subreddits with Images By thumbnail_caption\")\n",
    "filtered_captions_by_thumbnail = filtered_captions[\n",
    "\t(processed_with_captions_more[\"thumbnail_caption\"] != \"bruh\") &\n",
    "\t(~processed_with_captions_more[\"thumbnail_caption\"].isna() | ~ processed_with_captions_more[\n",
    "\t\t\"thumbnail_caption\"].isnull())\n",
    "\t]\n",
    "\n",
    "filtered_captions_by_thumbnail_display = filtered_captions_by_thumbnail.groupby(\"subreddit\").size().reset_index(\n",
    "\tname=\"count\")\n",
    "display(filtered_captions_by_thumbnail_display.sort_values(\"count\", ascending=False))\n",
    "print(f\"Total Records {filtered_captions_by_thumbnail_display['count'].sum()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "sources = [\n",
    "\t{\"name\": \"CityScapes\", \"data\": [\"CityPorn\"]},\n",
    "\t{\"name\": \"NatureScapes\", \"data\": [\"EarthPorn\"]},\n",
    "\t{\"name\": \"CosmicDiffusion\", \"data\": [\"spaceporn\"]},\n",
    "\t{\"name\": \"memes\", \"data\": [\"greentext\"]},\n",
    "\t{\"name\": \"SexyDiffusion\",\n",
    "\t \"data\": [\n",
    "\t\t \"sfwpetite\",\n",
    "\t\t \"selfies\",\n",
    "\t\t \"Amicute\",\n",
    "\t\t \"amihot\",\n",
    "\t\t \"AmIhotAF\",\n",
    "\t\t \"HotGirlNextDoor\",\n",
    "\t\t \"SFWNextDoorGirls\",\n",
    "\t\t \"SFWRedheads\"]\n",
    "\t }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'name': 'CityScapes', 'data': ['CityPorn']},\n {'name': 'NatureScapes', 'data': ['EarthPorn']},\n {'name': 'CosmicDiffusion', 'data': ['spaceporn']},\n {'name': 'memes', 'data': ['greentext']},\n {'name': 'SexyDiffusion',\n  'data': ['sfwpetite',\n   'selfies',\n   'Amicute',\n   'amihot',\n   'AmIhotAF',\n   'HotGirlNextDoor',\n   'SFWNextDoorGirls',\n   'SFWRedheads']}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "              name                                               data\n0       CityScapes                                         [CityPorn]\n1     NatureScapes                                        [EarthPorn]\n2  CosmicDiffusion                                        [spaceporn]\n3            memes                                        [greentext]\n4    SexyDiffusion  [sfwpetite, selfies, Amicute, amihot, AmIhotAF...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CityScapes</td>\n      <td>[CityPorn]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NatureScapes</td>\n      <td>[EarthPorn]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CosmicDiffusion</td>\n      <td>[spaceporn]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>memes</td>\n      <td>[greentext]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SexyDiffusion</td>\n      <td>[sfwpetite, selfies, Amicute, amihot, AmIhotAF...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sources)\n",
    "sources_df = pandas.DataFrame(sources)\n",
    "display(sources_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# from tqdm import dask\n",
    "# import pandas\n",
    "# import dask.dataframe as dd\n",
    "# import shutil\n",
    "# from PIL import Image\n",
    "# import pandas\n",
    "# def map_source_record(source_record: dict, filtered_records: pandas.DataFrame):\n",
    "# \tmodel_name: str = source_record.get('name')\n",
    "# \tsub_list: [str] = source_record.get('data')\n",
    "# \tout_path_for_model = os.path.join(\"out\", model_name)\n",
    "# \tos.makedirs(out_path_for_model, exist_ok=True)\n",
    "# \tddf = dd.from_pandas(filtered_records, npartitions=12)\n",
    "# \twith TqdmCallback(desc=\"map-inner-records\"):\n",
    "# \t\tddf.apply(lambda x: map_inner_record(x, sub_list, out_path_for_model), axis=1, meta=('str', object))\n",
    "# \t\tddf.compute()\n",
    "#\n",
    "#\n",
    "# def map_inner_record(inner_record: dict, sub_list: [str], out_path_for_model: str):\n",
    "# \tnew_out_records = []\n",
    "#\n",
    "# \tsub_reddit = inner_record.get('subreddit')\n",
    "# \tif sub_reddit in sub_list:\n",
    "# \t\tfile_name = inner_record.get(\"original_image\")\n",
    "# \t\ttext = inner_record.get(\"text\")\n",
    "# \t\ttext_2 = inner_record.get(\"thumbnail_caption\")\n",
    "#\n",
    "# \t\tvalid_image_path = inner_record.get(\"original_image\")\n",
    "# \t\ttry:\n",
    "# \t\t\ttemp_opened_image: Image = Image.open(valid_image_path)\n",
    "# \t\t\timage_size = temp_opened_image.size\n",
    "# \t\t\ttemp_opened_image.close()\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\tprint(f\"Invalid Image {valid_image_path}, {e}\")\n",
    "# \t\t\treturn None\n",
    "#\n",
    "# \t\tshutil.copy(valid_image_path, out_path_for_model)\n",
    "# \t\tnew_out_record = {\"file_name\": file_name, \"text\": [text, text_2]}\n",
    "# \t\tnew_out_records.append(new_out_record)\n",
    "#\n",
    "# \tfinal_out_records = pandas.DataFrame(new_out_records)\n",
    "# \tfinal_out_records.to_json(\"metadata.jsonl\", orient=\"records\", lines=True)\n",
    "# \tshutil.copy2(\"metadata.jsonl\", out_path_for_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# sources_df = pandas.DataFrame(sources)\n",
    "# sources_ddf = dd.from_pandas(sources_df, npartitions=12)\n",
    "# with TqdmCallback(desc=\"run-map-source-record\"):\n",
    "# \tsources_ddf.apply(lambda x: map_source_record(x, filtered_captions_by_thumbnail), axis=1, meta=('str', object))\n",
    "# \tsources_ddf.compute()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(\"out\"):\n",
    "\tshutil.rmtree(\"out\")\n",
    "\n",
    "if os.path.exists(\"out.zip\"):\n",
    "\tprint(\"Removing Old File\")\n",
    "\tos.remove(\"out.zip\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspaces\\General\\venv\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "D:\\workspaces\\General\\venv\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sources_df = pandas.DataFrame(sources)\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"out\"):\n",
    "\tshutil.rmtree(\"out\")\n",
    "\n",
    "if os.path.exists(\"out.zip\"):\n",
    "\tprint(\"Removing Old File\")\n",
    "\tos.remove(\"out.zip\")\n",
    "\n",
    "for item in sources:\n",
    "\tnew_records = []\n",
    "\tout_dir = os.path.join(\"out\", item['name'])\n",
    "\tos.makedirs(out_dir, exist_ok=True)\n",
    "\tfor record in filtered_captions_by_thumbnail.to_dict(orient='records'):\n",
    "\t\tvalid_image = record.get(\"original_image\")\n",
    "\t\ttry:\n",
    "\t\t\tsubreddit = record['subreddit']\n",
    "\t\t\tif subreddit in item['data']:\n",
    "\t\t\t\topened_image: Image = Image.open(valid_image)\n",
    "\t\t\t\tb = opened_image.size\n",
    "\t\t\t\topened_image.close()\n",
    "\n",
    "\n",
    "\t\t\t\tshutil.copy(valid_image, out_dir)\n",
    "\t\t\t\tout_record = {\"file_name\": record.get(\"file_name\"),\n",
    "\t\t\t\t\t\t\t  \"text\": [record.get(\"text\"), record.get(\"original_caption\")]}\n",
    "\t\t\t\tnew_records.append(out_record)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Invalid Image {valid_image}\")\n",
    "\t\t\tcontinue\n",
    "\tout_records = pandas.DataFrame(new_records)\n",
    "\tout_records.to_json(\"metadata.jsonl\", orient=\"records\", lines=True)\n",
    "\tshutil.move(\"metadata.jsonl\", out_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "!tar -a -c -f out.zip out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=507548422904-d8bsqsniuihb6a5uh7hesjo5s7brhs5u.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=online&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "drive = GoogleDrive(gauth)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: training_images.zip, id: 14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT\n",
      "GoogleDriveFile({'title': 'training_images.zip', 'parents': [{'kind': 'drive#parentReference', 'id': '1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni', 'selfLink': 'https://www.googleapis.com/drive/v2/files/14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT/parents/1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni', 'parentLink': 'https://www.googleapis.com/drive/v2/files/1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni', 'isRoot': False}], 'mimeType': 'application/x-zip-compressed', 'kind': 'drive#file', 'id': '14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT', 'etag': '\"MTY3Nzg5NDkxNjAwNQ\"', 'selfLink': 'https://www.googleapis.com/drive/v2/files/14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT', 'webContentLink': 'https://drive.google.com/uc?id=14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT&export=download', 'alternateLink': 'https://drive.google.com/file/d/14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT/view?usp=drivesdk', 'embedLink': 'https://drive.google.com/file/d/14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT/preview?usp=drivesdk', 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/application/x-zip-compressed', 'labels': {'starred': False, 'hidden': False, 'trashed': False, 'restricted': False, 'viewed': True}, 'copyRequiresWriterPermission': False, 'createdDate': '2023-03-04T01:55:16.005Z', 'modifiedDate': '2023-03-04T01:55:16.005Z', 'modifiedByMeDate': '2023-03-04T01:55:16.005Z', 'lastViewedByMeDate': '2023-03-04T01:55:16.005Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'version': '1', 'downloadUrl': 'https://www.googleapis.com/drive/v2/files/14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT?alt=media&source=downloadUrl', 'userPermission': {'kind': 'drive#permission', 'etag': '\"jd_R4btxxyZk8QyIexT0CHaDGb8\"', 'id': 'me', 'selfLink': 'https://www.googleapis.com/drive/v2/files/14Z7tp6TwqEX9-mIvV4lBlR0mD_uuTrUT/permissions/me', 'role': 'owner', 'type': 'user', 'pendingOwner': False}, 'originalFilename': 'training_images.zip', 'fileExtension': 'zip', 'md5Checksum': 'bd967b6226974ac92407928996331f04', 'fileSize': '5452598930', 'quotaBytesUsed': '5452598930', 'ownerNames': ['AJ Stangl'], 'owners': [{'kind': 'drive#user', 'displayName': 'AJ Stangl', 'picture': {'url': 'https://lh3.googleusercontent.com/a/AGNmyxY7pv1_ad1NClgYksHvoAzMmxWd1kq4L8wEVWDcbXM=s64'}, 'isAuthenticatedUser': True, 'permissionId': '08975006173809001511', 'emailAddress': 'ajstangl@gmail.com'}], 'lastModifyingUserName': 'AJ Stangl', 'lastModifyingUser': {'kind': 'drive#user', 'displayName': 'AJ Stangl', 'picture': {'url': 'https://lh3.googleusercontent.com/a/AGNmyxY7pv1_ad1NClgYksHvoAzMmxWd1kq4L8wEVWDcbXM=s64'}, 'isAuthenticatedUser': True, 'permissionId': '08975006173809001511', 'emailAddress': 'ajstangl@gmail.com'}, 'capabilities': {'canCopy': True, 'canEdit': True}, 'editable': True, 'copyable': True, 'writersCanShare': True, 'shared': False, 'explicitlyTrashed': False, 'appDataContents': False, 'headRevisionId': '0BypNHQz5kF2INm5iWXRyL0dmTW4xcHB4akRRMXV3NEdyYzRZPQ', 'spaces': ['drive']})\n"
     ]
    }
   ],
   "source": [
    "gfile = drive.CreateFile({\n",
    "\t'title': 'training_images.zip',\n",
    "\t'parents': [\n",
    "\t\t{\n",
    "\t\t\t'id': '1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni'\n",
    "\t\t}\n",
    "\t]\n",
    "})\n",
    "\n",
    "gfile.SetContentFile('out.zip')\n",
    "gfile.Upload()\n",
    "print('title: %s, id: %s' % (gfile['title'], gfile['id']))\n",
    "print(gfile)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
