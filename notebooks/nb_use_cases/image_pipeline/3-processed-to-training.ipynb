{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm.dask import TqdmCallback\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from shared_code.utility.spark.set_environ import set_azure_env\n",
    "\n",
    "set_azure_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "cb = TqdmCallback(desc=\"global-dd\")\n",
    "cb.register()\n",
    "tqdm.pandas(desc=\"global-pd\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class InnerProgressBar(tqdm):\n",
    "\tdef __init__(self, total, desc):\n",
    "\t\tsuper().__init__(desc=desc)\n",
    "\t\tself.total = total\n",
    "\t\tself.current = 0\n",
    "\n",
    "\tdef update_to(self):\n",
    "\t\tself.update(self.current)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_path = \"/data/parquet/\"\n",
    "parquet_process_data_path = data_path + \"processed_data.parquet\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating SD Models\n",
    "## SexyDiffusion\n",
    "- HotGirlNextDoor\n",
    "- sfwpetite\n",
    "- AmIhotAF\n",
    "- selfies\n",
    "- amihot\n",
    "- SFWNextDoorGirls\n",
    "- SFWRedheads\n",
    "- SFWPetite\n",
    "- Amicute\n",
    "\n",
    "## CityScapes\n",
    "- CityPorn\n",
    "\n",
    "## NatureScapes\n",
    "- EarthPorn\n",
    "\n",
    "## Memes\n",
    "- greentext\n",
    "\n",
    "The basic training line for a model is:\n",
    "```json lines\n",
    "{\"file_name\": \"0001.png\", \"text\": \"A cute cat.\"}\n",
    "{\"file_name\": \"0002.png\", \"text\": \"A cute dog.\"}\n",
    "{\"file_name\": \"0003.png\", \"text\": \"A cute bird.\"}\n",
    "```\n",
    "\n",
    "For each image we will do the following:\n",
    "- Caption the image with the caption (the actual caption from the other AI)\n",
    "- Use the thumbnail version of the image is to be used\n",
    "- Move all the images to a single folder along with the metadata.jsonl file\n",
    "\n",
    "A training line will look like:\n",
    "```json lines\n",
    "{\"file_name\": \"0001.png\", \"text\": \"A cute cat.\"}\n",
    "```\n",
    "\n",
    "Create small GPT Model for each SD model that will be used to generate the captions for the images based on what a user would say with the following translation:\n",
    "\n",
    "`<|startoftext|><|model|>SexyDiffusion<|model|><|prompt|>A cute cat<|prompt|><|text|>Foo<|text|><endoftext|>`\n",
    "\n",
    "This file will be named and stored in the following format:\n",
    "`training.txt`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from parquet /data/parquet/processed_data.parquet with Updated Thumbnail Captions\n"
     ]
    },
    {
     "data": {
      "text/plain": "                subreddit          file_name  \\\n0                CityPorn  4emw5uldib9a1.jpg   \n3                AmIhotAF  4xyb1vgbjb9a1.jpg   \n4               greentext  3mewbe0wjb9a1.jpg   \n5               spaceporn  7s5aafaqkb9a1.jpg   \n7               spaceporn  abojw7lqlb9a1.jpg   \n...                   ...                ...   \n16077           spaceporn  abwhhq0w8b9a1.jpg   \n16078           spaceporn  7hzipg1bab9a1.jpg   \n16079           greentext        bgho6WK.jpg   \n16081  trippinthroughtime        arCpzQ0.jpg   \n16084     HotGirlNextDoor  p6yewrl7eb9a1.jpg   \n\n                                                    text  \\\n0                                    New York in the fog   \n3                         Just looking for entertainment   \n4                                    Anon wants Elon cut   \n5                          Northern Lights above Lofoten   \n7                                          Viking Lights   \n...                                                  ...   \n16077           Polaris to Cassiopeia on a cloudy night.   \n16078  The hunt for habitable ocean worlds beyond our...   \n16079                        Anon does a little trolling   \n16081         He didn't shed light on the topic I guess.   \n16084                                             (IKTR)   \n\n                                          thumbnail_path  thumbnail_exists  \\\n0      D:\\data\\images\\CityPorn\\thumbnail\\4emw5uldib9a...              True   \n3      D:\\data\\images\\AmIhotAF\\thumbnail\\4xyb1vgbjb9a...              True   \n4      D:\\data\\images\\greentext\\thumbnail\\3mewbe0wjb9...              True   \n5      D:\\data\\images\\spaceporn\\thumbnail\\7s5aafaqkb9...              True   \n7      D:\\data\\images\\spaceporn\\thumbnail\\abojw7lqlb9...              True   \n...                                                  ...               ...   \n16077  D:\\data\\images\\spaceporn\\thumbnail\\abwhhq0w8b9...              True   \n16078  D:\\data\\images\\spaceporn\\thumbnail\\7hzipg1bab9...              True   \n16079     D:\\data\\images\\greentext\\thumbnail\\bgho6WK.jpg              True   \n16081  D:\\data\\images\\trippinthroughtime\\thumbnail\\ar...              True   \n16084  D:\\data\\images\\HotGirlNextDoor\\thumbnail\\p6yew...              True   \n\n                                         original_image  \\\n0             D:\\data\\images\\CityPorn\\4emw5uldib9a1.jpg   \n3             D:\\data\\images\\AmIhotAF\\4xyb1vgbjb9a1.jpg   \n4            D:\\data\\images\\greentext\\3mewbe0wjb9a1.jpg   \n5            D:\\data\\images\\spaceporn\\7s5aafaqkb9a1.jpg   \n7            D:\\data\\images\\spaceporn\\abojw7lqlb9a1.jpg   \n...                                                 ...   \n16077        D:\\data\\images\\spaceporn\\abwhhq0w8b9a1.jpg   \n16078        D:\\data\\images\\spaceporn\\7hzipg1bab9a1.jpg   \n16079              D:\\data\\images\\greentext\\bgho6WK.jpg   \n16081     D:\\data\\images\\trippinthroughtime\\arCpzQ0.jpg   \n16084  D:\\data\\images\\HotGirlNextDoor\\p6yewrl7eb9a1.jpg   \n\n       original_image_exists                              hash       id  \\\n0                       True  7a8d96e378c15c8ab8440ac311f12c11  1000cej   \n3                       True  e554c1ed7ffa2740436ac082068b2824  1000glf   \n4                       True  1dec3dabb5e46cde01855d06089c287a  1000j1n   \n5                       True  2c39ce1290fba541abd0b004b09da6b2  1000mjs   \n7                       True  0f72de47c69ff50eca5fa3990215f4ac  1000qpd   \n...                      ...                               ...      ...   \n16077                   True  f5973637fc56360c15818ba0ca1f7ffa   zzz6dp   \n16078                   True  5b22bea7582229c1f9b992176a2ca2c6   zzzcn5   \n16079                   True  df666b8b2ad543c77b3fdba89becda1a   zzzeoi   \n16081                   True  5007b937974ae333022c0c91b795ca09   zzzlbf   \n16084                   True  94dea288ddffb51eb1a786d469b59374   zzzu28   \n\n                                        original_caption  \\\n0      cars are parked on the side of the road in the...   \n3      blonde woman with blonde hair and tattoos on h...   \n4      a man with a beard and a beard sitting in fron...   \n5      a view of a view of a large green and purple a...   \n7      a scene of a boat is sitting on the shore of a...   \n...                                                  ...   \n16077     starrdust sky with a few stars and a few stars   \n16078  a picture taken from the earth's surface of th...   \n16079  a screenshote of a text message from a man who...   \n16081    a man in a red dress and a woman in a red dress   \n16084  blonde woman in black bikini top and black bik...   \n\n                                       thumbnail_caption  \n0      [[cars, NNS], [are, VBP], [parked, VBN], [on, ...  \n3      [[blonde, NNS], [woman, NN], [with, IN], [hair...  \n4      [[man, NN], [with, IN], [beard, NN], [and, CC]...  \n5      [[view, NN], [of, IN], [large, JJ], [green, JJ...  \n7      [[scene, NN], [of, IN], [boat, NN], [is, VBZ],...  \n...                                                  ...  \n16077  [[starrdust, NN], [sky, NN], [with, IN], [few,...  \n16078  [[picture, NN], [taken, VBN], [from, IN], [the...  \n16079  [[screenshote, NN], [of, IN], [text, JJ], [mes...  \n16081  [[man, NN], [in, IN], [red, JJ], [dress, NN], ...  \n16084  [[blonde, NN], [woman, NN], [in, IN], [black, ...  \n\n[11514 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>file_name</th>\n      <th>text</th>\n      <th>thumbnail_path</th>\n      <th>thumbnail_exists</th>\n      <th>original_image</th>\n      <th>original_image_exists</th>\n      <th>hash</th>\n      <th>id</th>\n      <th>original_caption</th>\n      <th>thumbnail_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CityPorn</td>\n      <td>4emw5uldib9a1.jpg</td>\n      <td>New York in the fog</td>\n      <td>D:\\data\\images\\CityPorn\\thumbnail\\4emw5uldib9a...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\CityPorn\\4emw5uldib9a1.jpg</td>\n      <td>True</td>\n      <td>7a8d96e378c15c8ab8440ac311f12c11</td>\n      <td>1000cej</td>\n      <td>cars are parked on the side of the road in the...</td>\n      <td>[[cars, NNS], [are, VBP], [parked, VBN], [on, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AmIhotAF</td>\n      <td>4xyb1vgbjb9a1.jpg</td>\n      <td>Just looking for entertainment</td>\n      <td>D:\\data\\images\\AmIhotAF\\thumbnail\\4xyb1vgbjb9a...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\AmIhotAF\\4xyb1vgbjb9a1.jpg</td>\n      <td>True</td>\n      <td>e554c1ed7ffa2740436ac082068b2824</td>\n      <td>1000glf</td>\n      <td>blonde woman with blonde hair and tattoos on h...</td>\n      <td>[[blonde, NNS], [woman, NN], [with, IN], [hair...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>greentext</td>\n      <td>3mewbe0wjb9a1.jpg</td>\n      <td>Anon wants Elon cut</td>\n      <td>D:\\data\\images\\greentext\\thumbnail\\3mewbe0wjb9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\greentext\\3mewbe0wjb9a1.jpg</td>\n      <td>True</td>\n      <td>1dec3dabb5e46cde01855d06089c287a</td>\n      <td>1000j1n</td>\n      <td>a man with a beard and a beard sitting in fron...</td>\n      <td>[[man, NN], [with, IN], [beard, NN], [and, CC]...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spaceporn</td>\n      <td>7s5aafaqkb9a1.jpg</td>\n      <td>Northern Lights above Lofoten</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\7s5aafaqkb9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\7s5aafaqkb9a1.jpg</td>\n      <td>True</td>\n      <td>2c39ce1290fba541abd0b004b09da6b2</td>\n      <td>1000mjs</td>\n      <td>a view of a view of a large green and purple a...</td>\n      <td>[[view, NN], [of, IN], [large, JJ], [green, JJ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>spaceporn</td>\n      <td>abojw7lqlb9a1.jpg</td>\n      <td>Viking Lights</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\abojw7lqlb9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\abojw7lqlb9a1.jpg</td>\n      <td>True</td>\n      <td>0f72de47c69ff50eca5fa3990215f4ac</td>\n      <td>1000qpd</td>\n      <td>a scene of a boat is sitting on the shore of a...</td>\n      <td>[[scene, NN], [of, IN], [boat, NN], [is, VBZ],...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16077</th>\n      <td>spaceporn</td>\n      <td>abwhhq0w8b9a1.jpg</td>\n      <td>Polaris to Cassiopeia on a cloudy night.</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\abwhhq0w8b9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\abwhhq0w8b9a1.jpg</td>\n      <td>True</td>\n      <td>f5973637fc56360c15818ba0ca1f7ffa</td>\n      <td>zzz6dp</td>\n      <td>starrdust sky with a few stars and a few stars</td>\n      <td>[[starrdust, NN], [sky, NN], [with, IN], [few,...</td>\n    </tr>\n    <tr>\n      <th>16078</th>\n      <td>spaceporn</td>\n      <td>7hzipg1bab9a1.jpg</td>\n      <td>The hunt for habitable ocean worlds beyond our...</td>\n      <td>D:\\data\\images\\spaceporn\\thumbnail\\7hzipg1bab9...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\spaceporn\\7hzipg1bab9a1.jpg</td>\n      <td>True</td>\n      <td>5b22bea7582229c1f9b992176a2ca2c6</td>\n      <td>zzzcn5</td>\n      <td>a picture taken from the earth's surface of th...</td>\n      <td>[[picture, NN], [taken, VBN], [from, IN], [the...</td>\n    </tr>\n    <tr>\n      <th>16079</th>\n      <td>greentext</td>\n      <td>bgho6WK.jpg</td>\n      <td>Anon does a little trolling</td>\n      <td>D:\\data\\images\\greentext\\thumbnail\\bgho6WK.jpg</td>\n      <td>True</td>\n      <td>D:\\data\\images\\greentext\\bgho6WK.jpg</td>\n      <td>True</td>\n      <td>df666b8b2ad543c77b3fdba89becda1a</td>\n      <td>zzzeoi</td>\n      <td>a screenshote of a text message from a man who...</td>\n      <td>[[screenshote, NN], [of, IN], [text, JJ], [mes...</td>\n    </tr>\n    <tr>\n      <th>16081</th>\n      <td>trippinthroughtime</td>\n      <td>arCpzQ0.jpg</td>\n      <td>He didn't shed light on the topic I guess.</td>\n      <td>D:\\data\\images\\trippinthroughtime\\thumbnail\\ar...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\trippinthroughtime\\arCpzQ0.jpg</td>\n      <td>True</td>\n      <td>5007b937974ae333022c0c91b795ca09</td>\n      <td>zzzlbf</td>\n      <td>a man in a red dress and a woman in a red dress</td>\n      <td>[[man, NN], [in, IN], [red, JJ], [dress, NN], ...</td>\n    </tr>\n    <tr>\n      <th>16084</th>\n      <td>HotGirlNextDoor</td>\n      <td>p6yewrl7eb9a1.jpg</td>\n      <td>(IKTR)</td>\n      <td>D:\\data\\images\\HotGirlNextDoor\\thumbnail\\p6yew...</td>\n      <td>True</td>\n      <td>D:\\data\\images\\HotGirlNextDoor\\p6yewrl7eb9a1.jpg</td>\n      <td>True</td>\n      <td>94dea288ddffb51eb1a786d469b59374</td>\n      <td>zzzu28</td>\n      <td>blonde woman in black bikini top and black bik...</td>\n      <td>[[blonde, NN], [woman, NN], [in, IN], [black, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11514 rows Ã— 11 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Reading from parquet {parquet_process_data_path} with Updated Thumbnail Captions\")\n",
    "processed_with_captions_more = pandas.read_parquet(parquet_process_data_path)\n",
    "display(processed_with_captions_more)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Subreddits with Images By original_caption\n"
     ]
    },
    {
     "data": {
      "text/plain": "              subreddit  count\n3             EarthPorn   1883\n2              CityPorn   1795\n13                memes   1180\n16            spaceporn   1101\n4                 Faces   1028\n9           SFWRedheads    984\n8      SFWNextDoorGirls    848\n12            greentext    713\n17   trippinthroughtime    478\n15            sfwpetite    288\n10               amihot    284\n7         RealGirls_SFW    262\n5       HotGirlNextDoor    262\n0              AmIhotAF    183\n11      fatsquirrelhate    122\n14              selfies     61\n1               Amicute     33\n6   OldLadiesBakingPies      9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>EarthPorn</td>\n      <td>1883</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CityPorn</td>\n      <td>1795</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>memes</td>\n      <td>1180</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>spaceporn</td>\n      <td>1101</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Faces</td>\n      <td>1028</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SFWRedheads</td>\n      <td>984</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SFWNextDoorGirls</td>\n      <td>848</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>greentext</td>\n      <td>713</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>trippinthroughtime</td>\n      <td>478</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sfwpetite</td>\n      <td>288</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>amihot</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RealGirls_SFW</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HotGirlNextDoor</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>AmIhotAF</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>fatsquirrelhate</td>\n      <td>122</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>selfies</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amicute</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>OldLadiesBakingPies</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records 11514\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering Subreddits with Images By original_caption\")\n",
    "filtered_captions = processed_with_captions_more[\n",
    "\t(processed_with_captions_more[\"original_caption\"] != \"bruh\") &\n",
    "\t(~processed_with_captions_more[\"original_caption\"].isna() | ~ processed_with_captions_more[\n",
    "\t\t\"original_caption\"].isnull())\n",
    "\t]\n",
    "\n",
    "filtered_captions_display = filtered_captions.groupby(\"subreddit\").size().reset_index(name=\"count\")\n",
    "\n",
    "display(filtered_captions_display.sort_values(\"count\", ascending=False))\n",
    "print(f\"Total Records {filtered_captions_display['count'].sum()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Subreddits with Images By thumbnail_caption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspaces\\General\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "data": {
      "text/plain": "              subreddit  count\n3             EarthPorn   1883\n2              CityPorn   1795\n13                memes   1180\n16            spaceporn   1101\n4                 Faces   1028\n9           SFWRedheads    984\n8      SFWNextDoorGirls    848\n12            greentext    713\n17   trippinthroughtime    478\n15            sfwpetite    288\n10               amihot    284\n7         RealGirls_SFW    262\n5       HotGirlNextDoor    262\n0              AmIhotAF    183\n11      fatsquirrelhate    122\n14              selfies     61\n1               Amicute     33\n6   OldLadiesBakingPies      9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>EarthPorn</td>\n      <td>1883</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CityPorn</td>\n      <td>1795</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>memes</td>\n      <td>1180</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>spaceporn</td>\n      <td>1101</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Faces</td>\n      <td>1028</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SFWRedheads</td>\n      <td>984</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SFWNextDoorGirls</td>\n      <td>848</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>greentext</td>\n      <td>713</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>trippinthroughtime</td>\n      <td>478</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sfwpetite</td>\n      <td>288</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>amihot</td>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RealGirls_SFW</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HotGirlNextDoor</td>\n      <td>262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>AmIhotAF</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>fatsquirrelhate</td>\n      <td>122</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>selfies</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amicute</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>OldLadiesBakingPies</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records 11514\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering Subreddits with Images By thumbnail_caption\")\n",
    "filtered_captions_by_thumbnail = filtered_captions[\n",
    "\t(processed_with_captions_more[\"thumbnail_caption\"] != \"bruh\") &\n",
    "\t(~processed_with_captions_more[\"thumbnail_caption\"].isna() | ~ processed_with_captions_more[\n",
    "\t\t\"thumbnail_caption\"].isnull())\n",
    "\t]\n",
    "\n",
    "filtered_captions_by_thumbnail_display = filtered_captions_by_thumbnail.groupby(\"subreddit\").size().reset_index(\n",
    "\tname=\"count\")\n",
    "display(filtered_captions_by_thumbnail_display.sort_values(\"count\", ascending=False))\n",
    "print(f\"Total Records {filtered_captions_by_thumbnail_display['count'].sum()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sources = [\n",
    "\t{\"name\": \"CityScapes\", \"data\": [\"CityPorn\"]},\n",
    "\t{\"name\": \"NatureScapes\", \"data\": [\"EarthPorn\"]},\n",
    "\t{\"name\": \"CosmicDiffusion\", \"data\": [\"spaceporn\"]},\n",
    "\t{\"name\": \"memes\", \"data\": [\"memes\"]},\n",
    "\t{\"name\": \"MemeDiffusion\", \"data\": [\"memes\", \"memes\"]},\n",
    "\t{\"name\": \"SexyDiffusion\",\n",
    "\t \"data\": [\"sfwpetite\", \"selfies\", \"Amicute\", \"amihot\", \"AmIhotAF\", \"HotGirlNextDoor\", \"SFWNextDoorGirls\",\n",
    "\t\t\t  \"SFWRedheads\"]\n",
    "\t }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'name': 'CityScapes', 'data': ['CityPorn']},\n {'name': 'NatureScapes', 'data': ['EarthPorn']},\n {'name': 'CosmicDiffusion', 'data': ['spaceporn']},\n {'name': 'memes', 'data': ['memes']},\n {'name': 'MemeDiffusion', 'data': ['memes', 'memes']},\n {'name': 'SexyDiffusion',\n  'data': ['sfwpetite',\n   'selfies',\n   'Amicute',\n   'amihot',\n   'AmIhotAF',\n   'HotGirlNextDoor',\n   'SFWNextDoorGirls',\n   'SFWRedheads']}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "              name                                               data\n0       CityScapes                                         [CityPorn]\n1     NatureScapes                                        [EarthPorn]\n2  CosmicDiffusion                                        [spaceporn]\n3            memes                                            [memes]\n4    MemeDiffusion                                     [memes, memes]\n5    SexyDiffusion  [sfwpetite, selfies, Amicute, amihot, AmIhotAF...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CityScapes</td>\n      <td>[CityPorn]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NatureScapes</td>\n      <td>[EarthPorn]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CosmicDiffusion</td>\n      <td>[spaceporn]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>memes</td>\n      <td>[memes]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MemeDiffusion</td>\n      <td>[memes, memes]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SexyDiffusion</td>\n      <td>[sfwpetite, selfies, Amicute, amihot, AmIhotAF...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sources)\n",
    "sources_df = pandas.DataFrame(sources)\n",
    "display(sources_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# from tqdm import dask\n",
    "# import pandas\n",
    "# import dask.dataframe as dd\n",
    "# import shutil\n",
    "# from PIL import Image\n",
    "# import pandas\n",
    "# def map_source_record(source_record: dict, filtered_records: pandas.DataFrame):\n",
    "# \tmodel_name: str = source_record.get('name')\n",
    "# \tsub_list: [str] = source_record.get('data')\n",
    "# \tout_path_for_model = os.path.join(\"out\", model_name)\n",
    "# \tos.makedirs(out_path_for_model, exist_ok=True)\n",
    "# \tddf = dd.from_pandas(filtered_records, npartitions=12)\n",
    "# \twith TqdmCallback(desc=\"map-inner-records\"):\n",
    "# \t\tddf.apply(lambda x: map_inner_record(x, sub_list, out_path_for_model), axis=1, meta=('str', object))\n",
    "# \t\tddf.compute()\n",
    "#\n",
    "#\n",
    "# def map_inner_record(inner_record: dict, sub_list: [str], out_path_for_model: str):\n",
    "# \tnew_out_records = []\n",
    "#\n",
    "# \tsub_reddit = inner_record.get('subreddit')\n",
    "# \tif sub_reddit in sub_list:\n",
    "# \t\tfile_name = inner_record.get(\"original_image\")\n",
    "# \t\ttext = inner_record.get(\"text\")\n",
    "# \t\ttext_2 = inner_record.get(\"thumbnail_caption\")\n",
    "#\n",
    "# \t\tvalid_image_path = inner_record.get(\"original_image\")\n",
    "# \t\ttry:\n",
    "# \t\t\ttemp_opened_image: Image = Image.open(valid_image_path)\n",
    "# \t\t\timage_size = temp_opened_image.size\n",
    "# \t\t\ttemp_opened_image.close()\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\tprint(f\"Invalid Image {valid_image_path}, {e}\")\n",
    "# \t\t\treturn None\n",
    "#\n",
    "# \t\tshutil.copy(valid_image_path, out_path_for_model)\n",
    "# \t\tnew_out_record = {\"file_name\": file_name, \"text\": [text, text_2]}\n",
    "# \t\tnew_out_records.append(new_out_record)\n",
    "#\n",
    "# \tfinal_out_records = pandas.DataFrame(new_out_records)\n",
    "# \tfinal_out_records.to_json(\"metadata.jsonl\", orient=\"records\", lines=True)\n",
    "# \tshutil.copy2(\"metadata.jsonl\", out_path_for_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# sources_df = pandas.DataFrame(sources)\n",
    "# sources_ddf = dd.from_pandas(sources_df, npartitions=12)\n",
    "# with TqdmCallback(desc=\"run-map-source-record\"):\n",
    "# \tsources_ddf.apply(lambda x: map_source_record(x, filtered_captions_by_thumbnail), axis=1, meta=('str', object))\n",
    "# \tsources_ddf.compute()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Old File\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(\"out\"):\n",
    "\tshutil.rmtree(\"out\")\n",
    "\n",
    "if os.path.exists(\"out.zip\"):\n",
    "\tprint(\"Removing Old File\")\n",
    "\tos.remove(\"out.zip\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspaces\\General\\venv\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "D:\\workspaces\\General\\venv\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (100228800 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "D:\\workspaces\\General\\venv\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sources_df = pandas.DataFrame(sources)\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"out\"):\n",
    "\tshutil.rmtree(\"out\")\n",
    "\n",
    "if os.path.exists(\"out.zip\"):\n",
    "\tprint(\"Removing Old File\")\n",
    "\tos.remove(\"out.zip\")\n",
    "\n",
    "for item in sources:\n",
    "\tnew_records = []\n",
    "\tout_dir = os.path.join(\"out\", item['name'])\n",
    "\tos.makedirs(out_dir, exist_ok=True)\n",
    "\tfor record in filtered_captions_by_thumbnail.to_dict(orient='records'):\n",
    "\t\tvalid_image = record.get(\"original_image\")\n",
    "\t\ttry:\n",
    "\t\t\tsubreddit = record['subreddit']\n",
    "\t\t\tif subreddit in item['data']:\n",
    "\t\t\t\topened_image: Image = Image.open(valid_image)\n",
    "\t\t\t\tb = opened_image.size\n",
    "\t\t\t\topened_image.close()\n",
    "\n",
    "\t\t\t\tshutil.copy(valid_image, out_dir)\n",
    "\t\t\t\tout_record = {\"file_name\": record.get(\"file_name\"),\n",
    "\t\t\t\t\t\t\t  \"text\": [record.get(\"text\"), record.get(\"original_caption\")]}\n",
    "\t\t\t\tnew_records.append(out_record)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Invalid Image {valid_image}\")\n",
    "\t\t\tcontinue\n",
    "\tout_records = pandas.DataFrame(new_records)\n",
    "\tout_records.to_json(\"metadata.jsonl\", orient=\"records\", lines=True)\n",
    "\tshutil.move(\"metadata.jsonl\", out_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "!tar -a -c -f out.zip out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=507548422904-d8bsqsniuihb6a5uh7hesjo5s7brhs5u.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=online&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "drive = GoogleDrive(gauth)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: training_images.zip, id: 1i0qWGBw5NYOHopge9kka5qLAj8yD62ri\n",
      "GoogleDriveFile({'title': 'training_images.zip', 'parents': [{'kind': 'drive#parentReference', 'id': '1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1i0qWGBw5NYOHopge9kka5qLAj8yD62ri/parents/1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni', 'parentLink': 'https://www.googleapis.com/drive/v2/files/1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni', 'isRoot': False}], 'mimeType': 'application/x-zip-compressed', 'kind': 'drive#file', 'id': '1i0qWGBw5NYOHopge9kka5qLAj8yD62ri', 'etag': '\"MTY3ODUwMzIxNzYxMQ\"', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1i0qWGBw5NYOHopge9kka5qLAj8yD62ri', 'webContentLink': 'https://drive.google.com/uc?id=1i0qWGBw5NYOHopge9kka5qLAj8yD62ri&export=download', 'alternateLink': 'https://drive.google.com/file/d/1i0qWGBw5NYOHopge9kka5qLAj8yD62ri/view?usp=drivesdk', 'embedLink': 'https://drive.google.com/file/d/1i0qWGBw5NYOHopge9kka5qLAj8yD62ri/preview?usp=drivesdk', 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/application/x-zip-compressed', 'labels': {'starred': False, 'hidden': False, 'trashed': False, 'restricted': False, 'viewed': True}, 'copyRequiresWriterPermission': False, 'createdDate': '2023-03-11T02:53:37.611Z', 'modifiedDate': '2023-03-11T02:53:37.611Z', 'modifiedByMeDate': '2023-03-11T02:53:37.611Z', 'lastViewedByMeDate': '2023-03-11T02:53:37.611Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'version': '1', 'downloadUrl': 'https://www.googleapis.com/drive/v2/files/1i0qWGBw5NYOHopge9kka5qLAj8yD62ri?alt=media&source=downloadUrl', 'userPermission': {'kind': 'drive#permission', 'etag': '\"_FSvk42ZSskkNSDo1lDW1jzEfwY\"', 'id': 'me', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1i0qWGBw5NYOHopge9kka5qLAj8yD62ri/permissions/me', 'role': 'owner', 'type': 'user', 'pendingOwner': False}, 'originalFilename': 'training_images.zip', 'fileExtension': 'zip', 'md5Checksum': '9509ed46c90f5088a5cf2e8fb5cf1d02', 'fileSize': '8688733759', 'quotaBytesUsed': '8688733759', 'ownerNames': ['AJ Stangl'], 'owners': [{'kind': 'drive#user', 'displayName': 'AJ Stangl', 'picture': {'url': 'https://lh3.googleusercontent.com/a/AGNmyxY7pv1_ad1NClgYksHvoAzMmxWd1kq4L8wEVWDcbXM=s64'}, 'isAuthenticatedUser': True, 'permissionId': '08975006173809001511', 'emailAddress': 'ajstangl@gmail.com'}], 'lastModifyingUserName': 'AJ Stangl', 'lastModifyingUser': {'kind': 'drive#user', 'displayName': 'AJ Stangl', 'picture': {'url': 'https://lh3.googleusercontent.com/a/AGNmyxY7pv1_ad1NClgYksHvoAzMmxWd1kq4L8wEVWDcbXM=s64'}, 'isAuthenticatedUser': True, 'permissionId': '08975006173809001511', 'emailAddress': 'ajstangl@gmail.com'}, 'capabilities': {'canCopy': True, 'canEdit': True}, 'editable': True, 'copyable': True, 'writersCanShare': True, 'shared': False, 'explicitlyTrashed': False, 'appDataContents': False, 'headRevisionId': '0BypNHQz5kF2IV2hqNlRqc0hLUkNlbnpZalU1QkEvYUk0emVFPQ', 'spaces': ['drive']})\n"
     ]
    }
   ],
   "source": [
    "gfile = drive.CreateFile({\n",
    "\t'title': 'training_images.zip',\n",
    "\t'parents': [\n",
    "\t\t{\n",
    "\t\t\t'id': '1-XhPy_rDB1TrStfC2NUO8q2pN0k1Njni'\n",
    "\t\t}\n",
    "\t]\n",
    "})\n",
    "\n",
    "gfile.SetContentFile('out.zip')\n",
    "gfile.Upload()\n",
    "print('title: %s, id: %s' % (gfile['title'], gfile['id']))\n",
    "print(gfile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
