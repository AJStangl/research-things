{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared_code.utility.schemas.spark_table_schema import image_table_schema, image_table_schema_with_caption_tokens, tokenize_caption_schema\n",
    "from shared_code.utility.spark.set_environ import *\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "set_azure_env()\n",
    "from shared_code.utility.storage.table import TableAdapter\n",
    "from shared_code.utility.schemas.spark_table_schema import tokenize_caption_schema, image_table_schema_with_caption_tokens\n",
    "spark = get_session('add-tokens-from-caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "table_name = \"training\"\n",
    "table_adapter: TableAdapter = TableAdapter()\n",
    "raw_data = table_adapter.get_all_entities(table_name)\n",
    "spark_df = spark.createDataFrame(list(raw_data), schema=image_table_schema)\n",
    "\n",
    "spark_df.createOrReplaceTempView(\"temp_table\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import sent_tokenize\n",
    "from collections import OrderedDict\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_nlk_tokens(text: str):\n",
    "\ttry:\n",
    "\t\ttokenizer = TweetTokenizer()\n",
    "\n",
    "\t\tfirst_sentence = sent_tokenize(text)[0]\n",
    "\n",
    "\t\t# remove numbers and tokenize the text\n",
    "\t\ttokenized = tokenizer.tokenize(first_sentence.translate({ord(ch): None for ch in '0123456789'}))\n",
    "\n",
    "\t\ttokenized = [i for i in tokenized if len(i) > 1]\n",
    "\n",
    "\t\ttokenized = list(OrderedDict.fromkeys(tokenized))\n",
    "\n",
    "\t\tpos_tagged_text = nltk.pos_tag(tokenized)\n",
    "\n",
    "\t\t# Extract all nouns, verbs and adverbs and append to the existing\n",
    "\t\tprompt_keywords = [i[0] for i in pos_tagged_text if i[1][:2] in ['NN', 'VB', 'RB']]\n",
    "\n",
    "\t\tprompt_keywords = list(OrderedDict.fromkeys(prompt_keywords))\n",
    "\n",
    "\t\treturn pos_tagged_text\n",
    "\texcept Exception as e:\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "def get_tokens_for_caption(caption: str):\n",
    "\ttry:\n",
    "\t\tif caption == 'NaN':\n",
    "\t\t\treturn None\n",
    "\t\tif caption == \"\":\n",
    "\t\t\treturn None\n",
    "\t\telse:\n",
    "\t\t\ttokens = get_nlk_tokens(caption)\n",
    "\t\t\tprint(tokens)\n",
    "\t\t\treturn tokens\n",
    "\texcept:\n",
    "\t\treturn None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "existing_images_not_curated = spark.sql(\"\"\"\n",
    "select * from\n",
    "temp_table\n",
    "where small_image is not null\n",
    "\tand small_image != ''\n",
    "\tand small_image != 'None'\n",
    "\tand small_image != 'Nan'\n",
    "\tand length(updated_caption)  > 5\n",
    "\tand length(caption) > 5\n",
    "\tand updated_caption not like '%Error%'\n",
    "\tand caption not like '%Error%'\n",
    "\tand exists == True\n",
    "\tand not curated\n",
    "\"\"\", schema=image_table_schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mapping Tokens To Caption ===\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\AJ Stangl\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 707, in readinto\n    raise\nsocket.timeout: timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPythonException\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m get_caption_image_name_udf \u001B[38;5;241m=\u001B[39m udf(get_tokens_for_caption, tokenize_caption_schema)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== Mapping Tokens To Caption ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m updated_captions_collection \u001B[38;5;241m=\u001B[39m \u001B[43mexisting_images_not_curated\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcaption_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_caption_image_name_udf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mupdated_caption\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_caption_image_name_udf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m out_frame \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(updated_captions_collection, schema\u001B[38;5;241m=\u001B[39mimage_table_schema_with_caption_tokens)\n\u001B[0;32m     11\u001B[0m display(out_frame\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m10\u001B[39m)\u001B[38;5;241m.\u001B[39mtoPandas())\n",
      "File \u001B[1;32mD:\\workspaces\\General\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:817\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    807\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m    808\u001B[0m \n\u001B[0;32m    809\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    814\u001B[0m \u001B[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001B[39;00m\n\u001B[0;32m    815\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    816\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[1;32m--> 817\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[1;32mD:\\workspaces\\General\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mD:\\workspaces\\General\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\AJ Stangl\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 707, in readinto\n    raise\nsocket.timeout: timed out\n"
     ]
    }
   ],
   "source": [
    "get_caption_image_name_udf = udf(get_tokens_for_caption, tokenize_caption_schema)\n",
    "\n",
    "print(\"=== Mapping Tokens To Caption ===\")\n",
    "updated_captions_collection = existing_images_not_curated \\\n",
    "\t.withColumn(\"caption_tokens\", get_caption_image_name_udf(\"updated_caption\")) \\\n",
    "\t.withColumn(\"text_tokens\", get_caption_image_name_udf(\"text\")) \\\n",
    "\t.collect()\n",
    "\n",
    "out_frame = spark.createDataFrame(updated_captions_collection, schema=image_table_schema_with_caption_tokens)\n",
    "\n",
    "display(out_frame.limit(10).toPandas())\n",
    "\n",
    "print(f\"Total Number Of Records From Pre-Processing {out_frame.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\" === Writing Records To Cloud parquet ===\")\n",
    "out_frame.write.parquet(\"D:\\\\data\\\\processed\\\\reddit_images_processed_with_caption_tokens.parquet\", mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
