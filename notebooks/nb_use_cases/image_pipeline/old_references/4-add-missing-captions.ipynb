{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from shared_code.utility.schemas.spark_table_schema import image_table_schema, tokenize_caption_schema\n",
    "from shared_code.utility.spark.set_environ import *\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, GPT2TokenizerFast, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "set_azure_env()\n",
    "\n",
    "from shared_code.utility.storage.table import TableAdapter\n",
    "\n",
    "spark_builder = SparkSession \\\n",
    "\t.builder \\\n",
    "\t.appName('add-missing-captions') \\\n",
    "\t.master(\"local[6]\") \\\n",
    "\t.config(\"spark.cores.max\", \"1\") \\\n",
    "\t.config(\"spark.executor.instances\", \"1\") \\\n",
    "\t.config(\"spark.executor.cores\", \"1\") \\\n",
    "\t.config(\"spark.executor.cores\", \"1\") \\\n",
    "\t.config(\"spark.executor.instances\", \"1\") \\\n",
    "\t.config(\"spark.driver.memory\", \"10g\") \\\n",
    "\t.config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "\t.config(\"spark.memory.offHeap.size\", \"10g\") \\\n",
    "\t.config(\"spark.executor.cores\", \"1\")\n",
    "\n",
    "spark = spark_builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Data Loaded For Captions ==\n"
     ]
    }
   ],
   "source": [
    "table_name = \"training\"\n",
    "table_adapter: TableAdapter = TableAdapter()\n",
    "raw_data = table_adapter.get_all_entities(table_name)\n",
    "spark_df = spark.createDataFrame(raw_data, schema=image_table_schema)\n",
    "print(\"== Data Loaded For Captions ==\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_vit_caption():\n",
    "\tmodel = VisionEncoderDecoderModel.from_pretrained(\"D:\\\\models\\\\vit-gpt2-image-captioning\")\n",
    "\tfeature_extractor = ViTFeatureExtractor.from_pretrained(\"D:\\\\models\\\\vit-gpt2-image-captioning\")\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(\"D:\\\\models\\\\vit-gpt2-image-captioning\")\n",
    "\treturn model, feature_extractor, tokenizer\n",
    "\n",
    "\n",
    "model, feature_extractor, tokenizer = get_vit_caption()\n",
    "\n",
    "\n",
    "def caption_image_vit(image_path: str) -> str:\n",
    "\ttry:\n",
    "\t\tmax_length = 32\n",
    "\n",
    "\t\tnum_beams = 4\n",
    "\n",
    "\t\tgen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "\t\timages = []\n",
    "\n",
    "\t\ti_image = Image.open(image_path)\n",
    "\t\tif i_image.mode != \"RGB\":\n",
    "\t\t\ti_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "\t\timages.append(i_image)\n",
    "\n",
    "\t\tprint(f\":: Predicting image: {image_path}\")\n",
    "\n",
    "\t\tpixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "\t\toutput_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "\t\tprint(f\":: Decoding output for image: {image_path}\")\n",
    "\t\tpredictions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "\t\tprediction = [prediction.strip() for prediction in predictions]\n",
    "\n",
    "\t\tprint(f\":: Completed prediction for image: {image_path}\")\n",
    "\t\tif len(prediction) > 0:\n",
    "\t\t\treturn prediction[0]\n",
    "\t\telse:\n",
    "\t\t\treturn None\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\":: Process Failed For {image_path} with {e}\")\n",
    "\t\treturn None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_default_caption():\n",
    "\tmodel = VisionEncoderDecoderModel.from_pretrained(\"D:\\\\models\\\\image-caption-generator\")\n",
    "\tfeature_extractor = ViTFeatureExtractor.from_pretrained(\"D:\\\\models\\\\image-caption-generator\")\n",
    "\ttokenizer = GPT2TokenizerFast.from_pretrained(\"D:\\\\models\\\\image-caption-generator\\\\tokenizer\")\n",
    "\treturn model, feature_extractor, tokenizer\n",
    "\n",
    "\n",
    "blip_model, blip_feature_extractor, blip_tokenizer = get_default_caption()\n",
    "\n",
    "\n",
    "def caption_image_blip(image_path: str) -> str:\n",
    "\ttry:\n",
    "\t\timg = Image.open(image_path)\n",
    "\t\tif img.mode != 'RGB':\n",
    "\t\t\timg = img.convert(mode=\"RGB\")\n",
    "\n",
    "\t\tpixel_values = blip_feature_extractor(images=[img], return_tensors=\"pt\").pixel_values\n",
    "\n",
    "\t\tmax_length = 128\n",
    "\t\tnum_beams = 4\n",
    "\n",
    "\t\t# get model prediction\n",
    "\t\toutput_ids = blip_feature_extractor.generate(pixel_values, num_beams=num_beams, max_length=max_length)\n",
    "\n",
    "\t\t# decode the generated prediction\n",
    "\t\tpredictions = blip_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\t\treturn predictions\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error in caption_image: {e}\")\n",
    "\t\treturn None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_caption_image_name(image_path: str, caption: str, row_key: str, partition_key: str):\n",
    "\tif not os.path.exists(image_path):\n",
    "\t\treturn \"\"\n",
    "\ttry:\n",
    "\t\tif caption is None or caption == \"\" or caption == \"NaN\" or caption == \"None\" or len(caption) < 5:\n",
    "\t\t\tvit_caption = caption_image_vit(image_path)\n",
    "\t\t\trecord = table_adapter.get_entity(\"training\", partition_key, row_key)\n",
    "\t\t\trecord[\"updated_caption\"] = vit_caption\n",
    "\t\t\ttable_adapter.upsert_entity_to_table(\"training\", record)\n",
    "\t\t\treturn caption\n",
    "\t\telse:\n",
    "\t\t\treturn caption\n",
    "\texcept:\n",
    "\t\treturn \"\"\n",
    "\n",
    "\n",
    "def get_blip_caption_image_name(image_path: str, caption: str, row_key: str, partition_key: str):\n",
    "\tif not os.path.exists(image_path):\n",
    "\t\treturn \"\"\n",
    "\ttry:\n",
    "\t\tif caption is None or caption == \"\" or caption == \"NaN\" or caption == \"None\" or len(caption) < 5:\n",
    "\t\t\t_blip_caption = caption_image_blip(image_path)\n",
    "\t\t\trecord = table_adapter.get_entity(\"training\", partition_key, row_key)\n",
    "\t\t\trecord[\"caption\"] = _blip_caption\n",
    "\t\t\ttable_adapter.upsert_entity_to_table(\"training\", record)\n",
    "\t\t\treturn _blip_caption\n",
    "\t\telse:\n",
    "\t\t\treturn caption\n",
    "\texcept:\n",
    "\t\treturn \"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to processes: 8600 image captions\n"
     ]
    }
   ],
   "source": [
    "remaining_captions = spark_df.select(\"image\", \"small_image\", \"updated_caption\", \"caption\", \"RowKey\", \"PartitionKey\", \"Exists\").collect()\n",
    "print(f\"Need to processes: {len(remaining_captions)} image captions\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining: 0 / 8600\n",
      "Remaining: 100 / 8600\n",
      "Remaining: 200 / 8600\n",
      "Remaining: 400 / 8600\n",
      "Remaining: 700 / 8600\n",
      "Remaining: 800 / 8600\n",
      "Remaining: 900 / 8600\n",
      "Remaining: 1000 / 8600\n",
      "Remaining: 1100 / 8600\n",
      "Remaining: 1300 / 8600\n",
      "Remaining: 1500 / 8600\n",
      "Remaining: 1600 / 8600\n",
      "Remaining: 1700 / 8600\n",
      "Remaining: 1900 / 8600\n",
      "Remaining: 2000 / 8600\n",
      "Remaining: 2100 / 8600\n",
      "Remaining: 2200 / 8600\n",
      "Remaining: 2300 / 8600\n",
      "Remaining: 2500 / 8600\n",
      "Remaining: 2600 / 8600\n",
      "Remaining: 2800 / 8600\n",
      "Remaining: 3000 / 8600\n",
      "Remaining: 3100 / 8600\n",
      "Remaining: 3200 / 8600\n",
      "Remaining: 3300 / 8600\n",
      "Remaining: 3400 / 8600\n",
      "Remaining: 3500 / 8600\n",
      "Remaining: 3600 / 8600\n",
      "Remaining: 3700 / 8600\n",
      "Remaining: 3800 / 8600\n",
      "Remaining: 3900 / 8600\n",
      "Remaining: 4000 / 8600\n",
      "Remaining: 4100 / 8600\n",
      "Remaining: 4200 / 8600\n",
      "File not found:  \n",
      "Remaining: 4300 / 8600\n",
      "Remaining: 4400 / 8600\n",
      "Remaining: 4500 / 8600\n",
      "Remaining: 4600 / 8600\n",
      "Remaining: 4700 / 8600\n",
      "Remaining: 4800 / 8600\n",
      "Remaining: 4900 / 8600\n",
      "Remaining: 5000 / 8600\n",
      "Remaining: 5100 / 8600\n",
      "Remaining: 5200 / 8600\n",
      "Remaining: 5300 / 8600\n",
      "Remaining: 5400 / 8600\n",
      "Remaining: 5500 / 8600\n",
      "Remaining: 5600 / 8600\n",
      "Remaining: 5700 / 8600\n",
      "Remaining: 5800 / 8600\n",
      "Remaining: 5900 / 8600\n",
      "Remaining: 6000 / 8600\n",
      "Remaining: 6100 / 8600\n",
      "Remaining: 6300 / 8600\n",
      "Remaining: 6500 / 8600\n",
      "Remaining: 6600 / 8600\n",
      ":: Process Failed For D:\\data\\images\\AmIhotAF\\thumbnail\\udyy41upqvz91.jpg with cannot identify image file 'D:\\\\data\\\\images\\\\AmIhotAF\\\\thumbnail\\\\udyy41upqvz91.jpg'\n",
      ":: Process Failed For D:\\data\\images\\AmIhotAF\\thumbnail\\udyy41upqvz91.jpg with cannot identify image file 'D:\\\\data\\\\images\\\\AmIhotAF\\\\thumbnail\\\\udyy41upqvz91.jpg'\n",
      "Remaining: 6800 / 8600\n",
      "Remaining: 7200 / 8600\n",
      "Remaining: 7300 / 8600\n",
      "Remaining: 7500 / 8600\n",
      "Remaining: 7700 / 8600\n",
      "Remaining: 7900 / 8600\n",
      "Remaining: 8000 / 8600\n",
      "Remaining: 8200 / 8600\n",
      "Remaining: 8300 / 8600\n",
      "Remaining: 8500 / 8600\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(list(remaining_captions)):\n",
    "\ttry:\n",
    "\t\tif not elem['Exists']:\n",
    "\t\t\tcontinue\n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\tprint(f\"Remaining: {i} / {len(remaining_captions)}\")\n",
    "\t\telem = elem.asDict()\n",
    "\t\t# print(elem)\n",
    "\n",
    "\t\tif elem is not None:\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif isinstance(elem, type(None)):\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif not os.path.exists(elem['image']):\n",
    "\t\t\tentity = table_adapter.get_entity(\"training\", elem['PartitionKey'], elem['RowKey'])\n",
    "\t\t\tentity['Exists'] = False\n",
    "\t\t\ttable_adapter.upsert_entity_to_table(\"training\", entity)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif len(elem['updated_caption']) > 5 and len(elem['caption']) > 5:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif os.path.exists(elem['small_image']):\n",
    "\t\t\tentity = table_adapter.get_entity(\"training\", elem['PartitionKey'], elem['RowKey'])\n",
    "\t\t\tblip_caption = None\n",
    "\t\t\tvit_result_caption = None\n",
    "\t\t\ttry:\n",
    "\t\t\t\tvit_result_caption = caption_image_vit(elem['small_image'])\n",
    "\t\t\t\tblip_caption = caption_image_vit(elem['small_image'])\n",
    "\t\t\texcept:\n",
    "\t\t\t\tresult = None\n",
    "\n",
    "\t\t\tentity[\"caption\"] = blip_caption\n",
    "\t\t\tentity[\"updated_caption\"] = vit_result_caption\n",
    "\t\t\ttable_adapter.upsert_entity_to_table(\"training\", entity)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"File not found: \", elem['small_image'])\n",
    "\t\t\tentity = table_adapter.get_entity(\"training\", elem['PartitionKey'], elem['RowKey'])\n",
    "\t\t\tentity['exists'] = False\n",
    "\t\t\ttable_adapter.upsert_entity_to_table(\"training\", entity)\n",
    "\texcept ConnectionResetError:\n",
    "\t\tprint(\"Connection Reset Error\")\n",
    "\t\ttable_adapter = TableAdapter()\n",
    "\t\tcontinue\n",
    "\texcept Exception as e:\n",
    "\t\t# print(e)\n",
    "\t\tcontinue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "refreshed = spark.createDataFrame(table_adapter.get_all_entities(\"training\"), schema=image_table_schema)\n",
    "\n",
    "display(refreshed.limit(10).toPandas())\n",
    "\n",
    "# refreshed.write.parquet(\"D:\\\\data\\\\processed\\\\reddit_images_processed_vit_caption.parquet\", mode=\"overwrite\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
